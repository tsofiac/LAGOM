epoch	training_loss	validation_loss	validation_token_accuracy	fraction_invalid	accuracy_top_1	lr-Adam
0	0.66777766					
1	0.5864487	0.6194455	0.84011626	0.07106855	0.0078125	
2	0.4436869	0.49288625	0.8544154	0.047631048	0.08694556	
3	0.41352403	0.42442083	0.864097	0.047631048	0.14264113	
4	0.3185461	0.3837748	0.8761896	0.03956653	0.14238912	
5	0.31611174	0.34380305	0.89488685	0.039818548	0.1265121	
6	0.30367014	0.3114875	0.9065354	0.07862903	0.17414315	
7	0.21867193	0.2928661	0.910939	0.031502016	0.19027218	
8	0.1346034	0.27991676	0.91335803	0.031502016	0.18220766	
9	0.14340489	0.2571446	0.9192953	0.039314516	0.23034275	
10	0.12620921	0.2562748	0.92171574	0.05519153	0.26990926	
11	0.088236645	0.23971234	0.92677045	0.055443548	0.32535282	
12	0.13065144	0.24557821	0.9300613	0.047631048	0.37298387	
13	0.111020915	0.2465023	0.9333616	0.04737903	0.38860887	
14	0.07051236	0.2544148	0.9324844	0.03125	0.35710686	
15	0.07251847	0.26996028	0.933145	0.103074595	0.35710686	
16	0.086449794	0.2591319	0.93095064	0.06325605	0.34879032	0.0009951295
17	0.08534251	0.2867291	0.9307178	0.17439516	0.3016633	0.0009951295
18	0.06774261	0.32235932	0.917763	0.19052419	0.3092238	0.0009951295
19	0.10354079	0.29272282	0.93028194	0.22933468	0.22958669	0.0009951295
20	0.071543574	0.2629217	0.93181974	0.11038306	0.30997983	0.0009951295
21	0.04061201	0.2608418	0.9340141	0.055695564	0.32560483	0.0009951295
22	0.05335951	0.28741324	0.93423605	0.070564516	0.3578629	0.0009951295
23	0.038522497	0.29351854	0.93775845	0.055695564	0.40549394	0.0009951295
24	0.018217118	0.30009174	0.93952364	0.047631048	0.41355845	0.0009951295
25	0.021339588	0.30636314	0.93819296	0.05519153	0.39742944	0.0009951295
26	0.04628442	0.31339848	0.93908095	0.03175403	0.39742944	0.0009951295
27	0.024797365	0.31901884	0.9410614	0.03175403	0.43674394	0.0009951295
28	0.0125157945	0.32644606	0.94171524	0.023689516	0.43649194	0.0009951295
29	0.007547264	0.33592498	0.93929213	0.023689516	0.42086694	0.0009951295
30	0.019462842	0.34277144	0.9384136	0.039314516	0.42893144	0.0009951295
31	0.008970482	0.35330713	0.93819296	0.047127016	0.42867944	0.0009951295
32	0.03377219	0.35585836	0.938419	0.047127016	0.44430444	0.0009951295
33	0.027414707	0.35437194	0.9399595	0.047127016	0.45236894	0.0004725233
34	0.014724095	0.35725176	0.9401761	0.047127016	0.43649194	0.0004725233
35	0.014354714	0.3582995	0.9412753	0.07106855	0.42061493	0.0004725233
36	0.008348054	0.36446655	0.9401802	0.07106855	0.41280243	0.0004725233
37	0.010894952	0.3686266	0.9397402	0.07106855	0.41280243	0.0004725233
38	0.010633644	0.3717739	0.94083536	0.06300403	0.43623993	0.0004725233
39	0.009089196	0.3737895	0.9410533	0.05519153	0.44405243	0.0004725233
40	0.016699983	0.37451223	0.94039536	0.05519153	0.44405243	0.0004725233
41	0.004358134	0.3741508	0.9406147	0.05519153	0.44405243	0.0004725233
42	0.0064403303	0.37506953	0.940834	0.05519153	0.45211694	0.0004725233
43	0.0025265072	0.37664285	0.94083536	0.06325605	0.44430444	0.0004725233
44	0.004844351	0.37803912	0.94039536	0.06325605	0.44430444	0.0004725233
45	0.00651579	0.37896734	0.940834	0.06325605	0.44430444	0.0004725233
46	0.011662704	0.37932992	0.940834	0.06325605	0.44430444	0.0004725233
47	0.02067319	0.37952864	0.940834	0.055443548	0.44430444	0.0004725233
48	0.012054675	0.37967592	0.940834	0.055443548	0.44430444	0.0004725233
49	0.012157942	0.3796736	0.940834	0.055443548	0.44430444	2.2482898e-07
